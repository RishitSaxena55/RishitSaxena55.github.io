<!DOCTYPE html>
<html>

<head>
    <style>
        body {
            font-family: Arial, sans-serif;
            font-size: 11pt;
            color: #000;
            line-height: 1.6;
            max-width: 750px;
        }

        p {
            margin-bottom: 1em;
        }

        a {
            color: #1155cc;
            text-decoration: underline;
        }

        .subject {
            font-weight: bold;
            margin-bottom: 20px;
            border-bottom: 1px solid #eee;
            padding-bottom: 10px;
        }

        .highlight {
            background: #fef3c7;
            padding: 12px;
            border-left: 4px solid #f59e0b;
            margin: 15px 0;
        }

        .divider {
            border: none;
            border-top: 1px solid #eee;
            margin: 20px 0;
        }

        table {
            border-collapse: collapse;
            width: 100%;
            margin: 15px 0;
        }

        th,
        td {
            border: 1px solid #ddd;
            text-align: left;
            padding: 10px;
            vertical-align: top;
        }

        th {
            background: #f8f9fa;
        }
    </style>
</head>

<body>

    <div class="subject">
        <strong>To:</strong> tjavidi@ucsd.edu<br><br>
        <strong>Subject:</strong> Summer Internship: Early Exit Networks + Adaptive Inference — IIIT Guwahati
    </div>

    <p>Dear Professor Javidi,</p>

    <p>I am <strong>Rishit Saxena</strong>, a second-year CS student at <strong>IIIT Guwahati</strong>, India (CPI:
        9.23/10). Your work on <strong>Active Learning and Sequential Information Acquisition</strong> resonates with my
        research on <strong>Early Exit Networks</strong> — where instance-dependent computation paths allow adaptive
        processing based on sample difficulty. I see a deep connection between your theoretical framework for optimal
        query complexity and my practical implementation of adaptive inference.</p>

    <p>I am writing to express my interest in joining your lab for <strong>Summer 2026 (May–August)</strong>.</p>

    <hr class="divider">

    <div class="highlight">
        <strong>Why Your Research Resonates:</strong><br>
        Your work on <strong>"Active Learning from Imperfect Labelers"</strong> (NeurIPS 2016) and <strong>"Active
            Learning with Logged Data"</strong> (ICML 2018) addresses the fundamental challenge of acquiring information
        efficiently under uncertainty. My Early Exit Networks implement a similar principle at inference time — easy
        samples exit early (requiring less "queries" through the network), while hard samples use the full model.
        This is adaptive computation as a resource allocation problem, which I believe connects to your
        information-theoretic framework.
    </div>

    <p><strong>My Relevant Experience:</strong></p>

    <table>
        <tr>
            <th style="width: 25%;">Project</th>
            <th>What I Built</th>
            <th>Connection to Your Work</th>
        </tr>
        <tr>
            <td><strong><a href="https://github.com/RishitSaxena55/EENet">EENet: Early Exit Networks</a></strong></td>
            <td>Dynamic neural networks with intermediate exit branches reducing inference latency by
                <strong>40%</strong>.
                Instance-dependent computation paths for adaptive complexity.</td>
            <td><strong>Adaptive resource allocation = active learning at inference!</strong></td>
        </tr>
        <tr>
            <td><strong><a href="https://github.com/RishitSaxena55/TorchiFy-v1">TorchiFy</a></strong></td>
            <td>PyTorch-like DL framework from scratch: Autograd engine, Multi-Head Attention, custom backward passes
            </td>
            <td><strong>Systems-level understanding for algorithm implementation</strong></td>
        </tr>
        <tr>
            <td><strong><a href="https://github.com/RishitSaxena55/GBC-Net-">GBC-Net</a></strong></td>
            <td>Medical imaging with <strong>Visual Acuity Curriculum Learning</strong> — sequential training strategy
            </td>
            <td><strong>Curriculum as sequential information acquisition</strong></td>
        </tr>
        <tr>
            <td><strong><a href="https://github.com/RishitSaxena55/dms-mini-pro">DMS Mini</a></strong></td>
            <td>INT8 quantization pipeline achieving <strong>~4.5ms CPU inference</strong></td>
            <td><strong>Efficient inference under resource constraints</strong></td>
        </tr>
    </table>

    <p><strong>What Excites Me About Your Lab:</strong></p>
    <ul>
        <li><strong>Active Learning theory</strong> — algorithmic foundations for efficient data acquisition</li>
        <li><strong>Gaussian Process Bandits with Adaptive Discretization</strong> (2018)</li>
        <li><strong>TILOS AI Institute</strong> — Learning-enabled Optimization at Scale</li>
        <li><strong>The bridge between theory and practice</strong> — "we often prove theorems but we also build"</li>
    </ul>

    <p><strong>Links:</strong> <a href="https://rishitsaxena55.github.io">Portfolio</a> | <a
            href="https://github.com/RishitSaxena55">GitHub</a> | <a
            href="https://linkedin.com/in/rishit-saxena-12922531b">LinkedIn</a></p>

    <p>Thank you for your time. I understand you receive many emails and may not be able to respond individually — but I
        wanted to express my genuine interest in the intersection of information-theoretic foundations and practical
        machine learning systems.</p>

    <p>Best regards,<br>
        <strong>Rishit Saxena</strong><br>
        rishitsaxena55@gmail.com | +91 7082968644
    </p>

</body>

</html>