<!DOCTYPE html>
<html>

<head>
    <style>
        body {
            font-family: Arial, sans-serif;
            font-size: 11pt;
            color: #000;
            line-height: 1.6;
            max-width: 750px;
        }

        p {
            margin-bottom: 1em;
        }

        a {
            color: #1155cc;
            text-decoration: underline;
        }

        .subject {
            font-weight: bold;
            margin-bottom: 20px;
            border-bottom: 1px solid #eee;
            padding-bottom: 10px;
        }

        .highlight {
            background: #fef3c7;
            padding: 12px;
            border-left: 4px solid #f59e0b;
            margin: 15px 0;
        }

        .divider {
            border: none;
            border-top: 1px solid #eee;
            margin: 20px 0;
        }

        table {
            border-collapse: collapse;
            width: 100%;
            margin: 15px 0;
        }

        th,
        td {
            border: 1px solid #ddd;
            text-align: left;
            padding: 10px;
            vertical-align: top;
        }

        th {
            background: #f8f9fa;
        }
    </style>
</head>

<body>

    <div class="subject">
        <strong>To:</strong> zhijian@ucsd.edu<br><br>
        <strong>Subject:</strong> Summer Internship: Built INT8 Quantization Pipeline + Early Exit Networks — IIIT
        Guwahati
    </div>

    <p>Dear Professor Liu,</p>

    <p>I am <strong>Rishit Saxena</strong>, a second-year CS student at <strong>IIIT Guwahati</strong>, India (CPI:
        9.23/10). Your work on <strong>efficient ML</strong> — especially HAQ (hardware-aware quantization) and its
        integration into Intel's OpenVINO — deeply resonates with my research focus. I've implemented <strong>INT8
            quantization pipelines</strong> achieving 4.5ms CPU inference and <strong>Early Exit Networks</strong>
        reducing latency by 40%.</p>

    <p>I am writing to express my interest in joining your lab for <strong>Summer 2026 (May–August)</strong>.</p>

    <hr class="divider">

    <div class="highlight">
        <strong>Why Your Research Deeply Resonates:</strong><br>
        Your HAQ paper showed me that quantization isn't just about bit-reduction — it's about <strong>hardware-aware
            search</strong> for optimal bit-widths per layer. I've applied this philosophy in my DMS Mini project,
        where I implemented INT8 quantization with ONNX/TFLite to achieve <strong>~4.5ms CPU inference</strong> on
        edge devices. Your work on TinyML and Song Han's AMC inspired my Early Exit Networks research, where I
        designed <strong>instance-adaptive computation paths</strong> that exit early for easy samples.
    </div>

    <p><strong>My Directly Relevant Experience:</strong></p>

    <table>
        <tr>
            <th style="width: 25%;">Project</th>
            <th>What I Built</th>
            <th>Connection to Your Work</th>
        </tr>
        <tr>
            <td><strong><a href="https://github.com/RishitSaxena55/EENet">EENet</a></strong></td>
            <td>Early Exit Networks with intermediate classifiers, reducing inference latency by <strong>40%</strong>.
                Instance-dependent computation paths for adaptive complexity.</td>
            <td><strong>Directly aligned with efficient inference!</strong></td>
        </tr>
        <tr>
            <td><strong><a href="https://github.com/RishitSaxena55/dms-mini-pro">DMS Mini</a></strong></td>
            <td><strong>INT8 quantization pipeline</strong> (ONNX/TFLite) achieving ~4.5ms CPU inference. Optimized
                EmotionCNN (~320K params) for edge deployment.</td>
            <td><strong>HAQ philosophy in practice!</strong></td>
        </tr>
        <tr>
            <td><strong><a href="https://github.com/RishitSaxena55/TorchiFy-v1">TorchiFy</a></strong></td>
            <td>PyTorch-like DL framework from scratch: Autograd engine, Multi-Head Attention, custom backward passes
            </td>
            <td><strong>Systems-level understanding</strong></td>
        </tr>
        <tr>
            <td><strong><a href="https://github.com/RishitSaxena55/gaussian-splatting-pytorch">3D Gaussian
                        Splatting</a></strong></td>
            <td>Differentiable rasterizer with custom CUDA-style backward passes</td>
            <td>SPVCNN++ (3D perception)</td>
        </tr>
    </table>

    <p><strong>What Excites Me About Your Lab:</strong></p>
    <ul>
        <li><strong>HAQ</strong> — Hardware-aware quantization (integrated into OpenVINO!)</li>
        <li><strong>NVILA / SparseVILA</strong> — Efficient visual language models</li>
        <li><strong>SparseLoRA</strong> — Accelerating LLM fine-tuning with contextual sparsity</li>
        <li><strong>SPVCNN++</strong> — 1st place nuScenes LiDAR Segmentation (3D perception)</li>
        <li><strong>Song Han lineage</strong> — AMC, ProxylessNAS, TinyML</li>
    </ul>

    <p><strong>Links:</strong> <a href="https://rishitsaxena55.github.io">Portfolio</a> | <a
            href="https://github.com/RishitSaxena55">GitHub</a> | <a
            href="https://linkedin.com/in/rishit-saxena-12922531b">LinkedIn</a></p>

    <p>Thank you for your time. Having implemented both quantization pipelines and early exit networks from scratch, I
        would be excited to contribute to your lab's research on efficient ML systems. I will also be submitting the
        intern form on your website.</p>

    <p>Best regards,<br>
        <strong>Rishit Saxena</strong><br>
        rishitsaxena55@gmail.com | +91 7082968644
    </p>

</body>

</html>