# Zhijian Liu — Form Application Responses

## Basic Info

| Field | Response |
|-------|----------|
| **Email** | rishitsaxena55@gmail.com |
| **Name** | Rishit Saxena |
| **Resume** | Upload the Zhijian Liu tailored resume (PDF) |

---

## Academic Info

| Field | Response |
|-------|----------|
| **School** | Indian Institute of Information Technology Guwahati |
| **Academic Program** | Undergraduate |
| **Major** | Computer science |
| **Year of Study** | 2nd year |
| **Expected Graduation** | May 2028 |

---

## Post-Graduation & Availability

| Field | Response |
|-------|----------|
| **Post-Graduation Plans** | PhD program |
| **Start Date** | May 1, 2026 |
| **Work Format** | Remote *(or "Either" if you're open to travel)* |
| **Duration** | 3 months *(Summer 2026)* |
| **Weekly Hours** | 40 hours or more |

---

## Research Interest (Copy-Paste Ready)

```
My research interests align closely with efficient machine learning and systems. I have hands-on experience in:

1. **Early Exit Networks (EENet)**: Designed dynamic neural networks with intermediate exit branches that reduce inference latency by 40%. Instance-dependent computation paths allow adaptive complexity — easy samples exit early, hard samples use the full network.

2. **INT8 Quantization Pipelines**: Built a production-ready perception system achieving ~4.5ms CPU inference via INT8 quantization (ONNX/TFLite). This is hardware-aware quantization in practice — directly inspired by HAQ.

3. **Systems-Level Understanding**: Implemented TorchiFy, a PyTorch-like deep learning framework from scratch with a complete Autograd engine, Multi-Head Attention, and custom backward passes.

I'm particularly excited by your work on HAQ (integrated into Intel's OpenVINO), NVILA/SparseVILA for efficient VLMs, and SPVCNN++ for 3D perception. The intersection of neural architecture search and hardware-aware optimization is precisely where I want to contribute.
```

---

## Background (Copy-Paste Ready)

```
**Research Experience:**
- Research Intern at IIIT Guwahati (May 2025 – Present): Researching Early Exit Networks for efficient inference on resource-constrained platforms. Developed INT8 quantization pipelines achieving 4.5ms CPU inference for real-time perception.

**Relevant Projects:**
1. EENet: Early Exit Networks reducing inference latency by 40%
2. DMS Mini: INT8 quantization pipeline (ONNX/TFLite), ~4.5ms CPU inference
3. TorchiFy: PyTorch-like DL framework from scratch (Autograd, MHA, custom backward passes)
4. 3D Gaussian Splatting: Differentiable rasterizer from first principles
5. NeRF: Pure PyTorch implementation with stratified/hierarchical sampling

**Relevant Courses:**
- Data Structures and Algorithms
- Linear Algebra
- Probability and Statistics
- Machine Learning (self-study via Stanford CS229, deeplearning.ai)
- Deep Learning for Computer Vision (self-study via Stanford CS231N)

**Skills:** Python, C++, PyTorch, TensorFlow, ONNX, TFLite, CUDA, Quantization, Edge Deployment
```

---

## Anything Else (Optional — Copy-Paste Ready)

```
I noticed your commitment to mentorship for underrepresented groups and students in need. As a first-generation college student from India, I deeply appreciate this initiative.

I've also been following your recent work on SparseLoRA and Fast-dLLM — the direction of efficient LLM inference is exciting, and I'd love to contribute if there are opportunities in that space.

GitHub: https://github.com/RishitSaxena55
Portfolio: https://rishitsaxena55.github.io
```

---

## Summary Checklist

| Item | Status |
|------|--------|
| Upload tailored resume (PDF) | ⬜ |
| Upload transcript (PDF) | ⬜ |
| Fill in text fields | ⬜ |
| Submit form | ⬜ |
| **DO NOT send follow-up email** | ✅ (he says no follow-up needed) |
